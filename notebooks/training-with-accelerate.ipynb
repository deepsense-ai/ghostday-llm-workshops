{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1207c5aaa5df4118a1af00d50d2e6b67":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17cfae26cf3f427a9293f8e9d91e3592":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7068f9b8fa7841f8be47e3d72bb3e049","placeholder":"​","style":"IPY_MODEL_e3b73dce015c4c64b499fff0c80e2bbd","value":" 2/2 [00:00&lt;00:00, 58.12it/s]"}},"2c76773648af4ae5aa6ff86c85ffe494":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7d417fe59ae748cb9305f1fab5cac0b1","IPY_MODEL_39e32c6502914f93962b211f7e3c0717","IPY_MODEL_17cfae26cf3f427a9293f8e9d91e3592"],"layout":"IPY_MODEL_9c6161f025594acab91dd6361ce18ce5"}},"39e32c6502914f93962b211f7e3c0717":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b42f245e22947bdb39587c4600e65a7","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b8c1b65b6bae4ee3a18e302a21a26d9f","value":2}},"4b42f245e22947bdb39587c4600e65a7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b74ee5b1cd9472593a310c92f2ab49f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7248b810b4a8482c9afb7fb93c7a533a","placeholder":"​","style":"IPY_MODEL_c1dd4d56605542f0af93a40f82ccf714","value":"100%"}},"5d450c2dfae44fdaa8229b9b9f7f3198":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7068f9b8fa7841f8be47e3d72bb3e049":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7248b810b4a8482c9afb7fb93c7a533a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d417fe59ae748cb9305f1fab5cac0b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b80d836ec5af4b99a4235ef8a2eea34b","placeholder":"​","style":"IPY_MODEL_8a0cfde997dc4f3cb0cc9d9a271a9855","value":"100%"}},"8a0cfde997dc4f3cb0cc9d9a271a9855":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"97c155881fbd441494b6840a2e6563f6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d450c2dfae44fdaa8229b9b9f7f3198","placeholder":"​","style":"IPY_MODEL_ac6c4fba09264a0ea281021484a62bd8","value":" 2/2 [00:00&lt;00:00, 50.24it/s]"}},"9c6161f025594acab91dd6361ce18ce5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a32e40fc891c4c6980bab9d56f940e70":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac6c4fba09264a0ea281021484a62bd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b7cfe59369324991be4e9daff711bad5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5b74ee5b1cd9472593a310c92f2ab49f","IPY_MODEL_bc4ddc9f5b734dd9be52a905461fb2b4","IPY_MODEL_97c155881fbd441494b6840a2e6563f6"],"layout":"IPY_MODEL_a32e40fc891c4c6980bab9d56f940e70"}},"b80d836ec5af4b99a4235ef8a2eea34b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8c1b65b6bae4ee3a18e302a21a26d9f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bc4ddc9f5b734dd9be52a905461fb2b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1207c5aaa5df4118a1af00d50d2e6b67","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ccd3cb0072d2425f996c5da1ebfe3d3a","value":2}},"c1dd4d56605542f0af93a40f82ccf714":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ccd3cb0072d2425f996c5da1ebfe3d3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e3b73dce015c4c64b499fff0c80e2bbd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Large Language Models in limited hardware environments - training\n\nW notebooku wytrenujesz model enkodera na przykładowym zbiorze danych (_banking77_) przy użyciu metod optymalizacyjnych z biblioteki 🤗 Accelerate.\n\nWykonaj zadania oznaczone jako _ToDo_, uzupełniając kod w miejscach oznaczonych jako `#FIXME`","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m_feUQEoXDLa","outputId":"146af9f8-cd7a-4462-88bd-ca0f63874e9f"}},{"cell_type":"code","source":"!pip install transformers==4.28.1 torch==1.13.0 accelerate==0.18.0 datasets==2.1.0 evaluate==0.4.0 deepspeed==0.8.3","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import evaluate\nimport torch\nfrom datasets import load_dataset\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n\nfrom accelerate import Accelerator, DistributedType, notebook_launcher, DeepSpeedPlugin\nfrom tqdm import tqdm\n\nfrom typing import Dict","metadata":{"id":"dd2y_MfmXneH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataloaders(accelerator: Accelerator, model_id: str, batch_size: int):\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    datasets = load_dataset(\"banking77\")\n\n    def tokenize_function(examples):\n        outputs = tokenizer(examples[\"text\"], truncation=True, max_length=None)\n        return outputs\n    \n    # ToDo 1: Tokenize the dataset using main_process_first() context manager.\n    # https://huggingface.co/docs/accelerate/concept_guides/deferring_execution\n    ## with #FIXME: \n        tokenized_datasets = datasets.map(\n            tokenize_function,\n            batched=True,\n            remove_columns=[\"text\"],\n        )\n    \n    tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n\n    def collate_fn(examples):\n        pad_to_multiple_of = None\n        max_length = None\n\n        return tokenizer.pad(\n            examples,\n            padding=\"longest\",\n            max_length=max_length,\n            pad_to_multiple_of=pad_to_multiple_of,\n            return_tensors=\"pt\",\n        )\n\n    train_dataloader = DataLoader(\n        tokenized_datasets[\"train\"], shuffle=True, collate_fn=collate_fn, batch_size=batch_size, drop_last=True\n    )\n    eval_dataloader = DataLoader(\n        tokenized_datasets[\"test\"],\n        shuffle=False,\n        collate_fn=collate_fn,\n        batch_size=batch_size\n    )\n\n    return train_dataloader, eval_dataloader","metadata":{"id":"hsv2OpmTXlme"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Do uzupełnienia ToDo's w kodzie wykorzystaj [Accelerator API](https://huggingface.co/docs/accelerate/package_reference/accelerator)","metadata":{}},{"cell_type":"code","source":"def train_with_accelerate(train_args: Dict, model_id: str, mixed_precision: str=\"no\", ds_config: str=None):\n    \n    if ds_config is None:\n        # ToDo 2: Initialize accelerator (pass mixed_precision argument to it).\n        ## accelerator = #FIXME\n    else:\n        pass\n        # ToDO 17: Initialize accelerator. Pass both mixed_precision and deepspeed_plugin to it (use DeepSpeedPlugin).   \n        ## accelerator = #FIXME\n          \n    train_dataloader, eval_dataloader = get_dataloaders(accelerator, model_id, train_args[\"batch_size\"])\n    \n    model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=77)\n    # ToDo 3: Change the code below to to let accelerator handle the device placement.\n    device = torch.device(\"cuda\")\n    model = model.to(device)\n\n    optimizer = AdamW(params=model.parameters(), lr=train_args[\"lr\"])\n    gradient_accumulation_steps = train_args[\"gradient_accumulation_steps\"]\n    \n    lr_scheduler = get_linear_schedule_with_warmup(\n        optimizer=optimizer,\n        num_warmup_steps=train_args[\"num_warmup_steps\"],\n        num_training_steps=(len(train_dataloader) * train_args[\"num_epochs\"]) // gradient_accumulation_steps,\n    )\n    \n    # ToDo 4: Prepare all objects for distributed training. There is no specific order to remember, you just need \n    # to unpack the objects in the same order you gave them to the prepare method.\n    ## model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = #FIXME\n      \n    metric = evaluate.load(train_args[\"metric\"])\n\n    for epoch in range(train_args[\"num_epochs\"]):\n        model.train()\n        pbar = tqdm(train_dataloader)\n        \n        for step, batch in enumerate(pbar):\n            # ToDo 5: Change the code below to to let accelerator handle the device placement.\n            batch.to(device)\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss = loss / gradient_accumulation_steps\n            \n            # ToDo 6: Use the accelerator object for the backward pass.\n            ## FIXME\n            if step % gradient_accumulation_steps == 0:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                \n            pbar.set_description(f\"epoch {epoch} iter {step}: train loss {loss.item():.5f}.\")\n\n        model.eval()\n        for step, batch in enumerate(eval_dataloader):\n            # ToDo 7: Change the code below to to let accelerator handle the device placement.\n            batch.to(device)\n            with torch.no_grad():\n                outputs = model(**batch)\n            predictions = outputs.logits.argmax(dim=-1)\n            # ToDo 8: Gathers predictions and targets from used devices for metric calculation.\n            ## predictions, references = #FIXME\n            metric.add_batch(\n                predictions=predictions,\n                references=references,\n            )\n\n        eval_metric = metric.compute()\n        \n        accelerator.print(f\"epoch {epoch}:\", eval_metric)\n    \n    # ToDo 9: To save the model afterwards to use for inference wait for all of the processes to be aligned\n    # https://huggingface.co/docs/accelerate/v0.18.0/en/package_reference/accelerator#synchronicity-control\n    ## #FIXME\n    \n    # unwrap the model from any distributed wrapping that was performed\n    # https://huggingface.co/docs/accelerate/v0.18.0/en/package_reference/accelerator#synchronicity-control\n    ## #FIXME\n\n    # ToDo 10: Use save() instead of torch.save to save the model once (as all workers have the same model with the same weights now)\n    ## #FIXME","metadata":{"id":"kOiryCj-atUU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_ID = \"bert-large-cased\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_params = {\n    \"lr\": 2e-5, \n    \"num_epochs\": 5, \n    \"batch_size\": 32, \n    \"metric\": \"accuracy\", \n    \"gradient_accumulation_steps\": 8, \n    \"num_warmup_steps\": 100\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 1: Baseline implementation.\nModify to-dos from 1 and 10 in `get_dataloaders` and `train_with_accelerate` functions to run the baseline implementation of the training loop.","metadata":{}},{"cell_type":"code","source":"notebook_launcher(\n    train_with_accelerate, \n    args=(train_params, MODEL_ID, ), \n    num_processes=1\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: Distributed Data Parallelism.\nModify the cells below to execute the same previous training loop with Distribute Data Parallelism using 2 GPUs.","metadata":{}},{"cell_type":"code","source":"# ToDo 11: Adjust the batch size per GPU and the number of gradient accumulation steps for the current settings (2 GPUs) so that the global batch size is still 256.\ntrain_params = {\n    \"lr\": 2e-5, \n    \"num_epochs\": 5, \n    \"batch_size\": #FIXME, \n    \"metric\": \"accuracy\", \n    \"gradient_accumulation_steps\": #FIXME, \n    \"num_warmup_steps\": 100\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ToDo 12: Run training on 2 GPUs.\nnotebook_launcher(\n    train_with_accelerate, \n    args=(train_params, MODEL_ID, ), \n    num_processes=#FIXME\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: Mixed precision training.\nModify `ToDo 13` to run mixed precision training.","metadata":{}},{"cell_type":"code","source":"# ToDo 13: Pass proper mixed_precision argument to train_with_accelerate to run training in fp16.\nnotebook_launcher(\n    train_with_accelerate, \n    args=(train_params, MODEL_ID, #FIXME, ), \n    num_processes=#FIXME\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 4: ZeRO Stage 3.\n_To-dos_ od 14 do 17:\nPrzygotuj plik konfiguracyjny do uruchomienia optymalizacji ZeRO Stage 3. Sprawdź [dokumentację accelerate](https://huggingface.co/docs/accelerate/v0.18.0/en/usage_guides/deepspeed#deepspeed-config-file) w celu poznania szczegółów.","metadata":{}},{"cell_type":"code","source":"# ToDo 14: Adjust the batch size per GPU and the number of gradient accumulation steps.\ntrain_params = {\n    \"lr\": 2e-5, \n    \"num_epochs\": 5, \n    \"batch_size\": #FIXME, \n    \"metric\": \"accuracy\", \n    \"gradient_accumulation_steps\": #FIXME, \n    \"num_warmup_steps\": 100\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile ds_zero_config.json\n{\n    # ToDo 15: Enable zero_optimization stage 3 with CPU offload for both parameters and optimizer states.\n    ## \"zero_optimization\": FIXME  \n    \"gradient_accumulation_steps\": #FIXME,\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 100,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": #FIXME,\n    \"wall_clock_breakdown\": false\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ToDo 16: Run training with ZeRo Stage 3 optimizer.\nnotebook_launcher(\n    train_with_accelerate, \n    args=(train_params, MODEL_ID, \"fp16\", #FIXME), \n    num_processes=#FIXME\n)","metadata":{},"execution_count":null,"outputs":[]}]}