{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Large Language Models in limited hardware environments - inference\n\nW notebooku uruchomisz model dekodera na przyk≈Çadowym zadaniu ze zbioru danych ([UCI ML Drug Review dataset](https://www.kaggle.com/datasets/jessicali9530/kuc-hackathon-winter-2018)) przy u≈ºyciu metod optymalizacyjnych z biblioteki ü§ó Accelerate.\n\nWykonaj zadania oznaczone jako _ToDo_, uzupe≈ÇniajƒÖc kod w miejscach oznaczonych jako `#FIXME`","metadata":{}},{"cell_type":"code","source":"!pip install transformers==4.28.1 accelerate==0.18.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"\"\"\nLabel the sentiment towards the drug of the given drug review. The sentiment label should be \"positive\" or \"negative\".\n\nText: \"Great help with insomnia. Had no side effects\nSentiment: positive\n\nText: Worst drug on market for sleep and I can't get any help\nSentiment: negative\n\nText: I took Ambien and it didn't do anything at all.\nSentiment:\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference with Model Parallelism - _gpt2_","metadata":{}},{"cell_type":"code","source":"model_id = \"gpt2\"\n# ToDo 1: Modify the code below to run inference with Model Parallelism. \nmodel = AutoModelForCausalLM.from_pretrained(model_id)  # FIXME\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ToDo 2: Check the device_map that Accelerate picked by accessing the hf_device_map attribute of your model.\n# FIXME","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ToDo 3: The code below should work with no issues. However, it's good practice to move inputs to proper device prior to calling generate(). Do it in line below.\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids  ##FIXME\nlogits = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, max_new_tokens=1)\n\nprint(tokenizer.decode(logits[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference with Model Parallelism - _gpt-j_","metadata":{}},{"cell_type":"code","source":"# ToDo 4: Modify the code below to run inference with Model Parallelism. \n\nmodel_id = 'EleutherAI/gpt-j-6b'\nmodel = AutoModelForCausalLM.from_pretrained(model_id)  # FIXME\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# ToDo 5: If the model fails with CPU memory error - you need to use sharded version of the model - look for it on HuggingFace. Modify model_id and re-run the notebook.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ToDo 3: The code below should work with no issues. However, it's good practice to move inputs to proper device prior to calling generate(). Do it in line below.\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids  ##FIXME\nlogits = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, max_new_tokens=1)\n\nprint(tokenizer.decode(logits[0]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference with Model Parallelism and FP16 precision - _gpt-j_","metadata":{}},{"cell_type":"code","source":"# ToDo 6: Modify the code below to run inference with Model Parallelism and precision reduced to fp16\n\nmodel_id = # FIXME\ntokenizer_model_id = 'EleutherAI/gpt-j-6b'\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)  # FIXME\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_model_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ToDo 3: The code below should work with no issues. However, it's good practice to move inputs to proper device prior to calling generate(). Do it in line below.\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)  ##FIXME\nlogits = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, max_new_tokens=1)\n\nprint(tokenizer.decode(logits[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference with Model Parallelism - GPUs + CPU","metadata":{}},{"cell_type":"code","source":"# ToDo 7: Let's assume we would like to keep some memory free on all GPUs and on the CPU. \n# Create a code that will design a device map to keep memory utilization of GPUs and CPU under 10GBs\n# Hint: initialize a model with empty weights first\nfrom accelerate import infer_auto_device_map, init_empty_weights","metadata":{},"execution_count":null,"outputs":[]}]}